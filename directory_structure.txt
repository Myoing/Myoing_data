Myoing_data/
│
├── .github/                        # GitHub 관련 설정 디렉토리
│   └── workflows/                  # GitHub Actions 워크플로우
│       └── Deployment.yaml         # CI/CD 파이프라인 설정
│
├── .gitignore                      # Git 무시 파일 목록
│
├── code/                           # 코드 디렉토리
│   ├── __init__.py                 # 파이썬 패키지 초기화 파일
│   ├── filters.py                  # 필터링 모듈
│   ├── kakao_map_basic_crawler.py  # 카카오맵 기본 정보 크롤러
│   └── review_crawler.py           # 리뷰 크롤링 모듈
│
├── DB_code/                        # 데이터베이스 관련 코드 디렉토리
│   ├── __init__.py                 # 파이썬 패키지 초기화 파일
│   ├── database.py                 # 데이터베이스 연결 설정
│   ├── models.py                   # SQLAlchemy 모델 정의
│   └── migration.py                # 데이터 마이그레이션 스크립트
│
├── data/                           # 데이터 디렉토리
│   ├── 1_location_categories/      # 1단계: 지역별 카테고리 데이터
│   │
│   ├── 2_combined_location_categories/  # 2단계: 통합된 지역 카테고리 데이터
│   │
│   ├── 3_filtered_location_categories/  # 3단계: 필터링된 지역별 카테고리 데이터
│   │
│   ├── 4_filtered_all/                  # 4단계: 전체 필터링 데이터
│   │
│   ├── 5_filtered_clubs/                # 5단계: 필터링된 클럽 데이터
│   │
│   └── 6_reviews_about_4/               # 6단계: 리뷰 데이터
│
├── legacy/                               # 레거시 코드
│   ├── crawler/
│   │   ├── __init__.py
│   │   ├── basic_crawler.py
│   │   ├── config.py
│   │   ├── detail_crawler.py
│   │   ├── filter_utils.py
│   │   ├── main.py
│   │   └── review_crawler.py
│   │
│   └── kakao_api_test.py
│
├── Dockerfile                            # Docker 이미지 빌드 설정 파일
├── docker-compose.yaml                   # Docker Compose 설정 파일
├── LICENSE                               # 라이선스 파일
├── main.py                               # 메인 실행 파일 (CLI 실행용)
├── api.py                                # FastAPI 기반 API 서버 (웹 서비스용)
├── README.md                             # 프로젝트 설명 파일
├── requirements.txt                      # 의존성 패키지 목록
└── .env                                  # 환경 변수 설정 파일

## 프로젝트 실행 순서
1. CLI 방식: main.py 실행
   - 4단계 순차 진행:
     - 1단계: 카카오맵 기본 크롤링 (kakao_map_basic_crawler.py)
     - 2단계: 데이터 필터링 (filters.py)
     - 3단계: 리뷰 크롤링 (review_crawler.py)
     - 4단계: DB 마이그레이션 (migration.py)

2. API 방식: api.py 실행
   - FastAPI 기반 웹 서버 시작 (포트: 7070)
   - 다음 API 엔드포인트 제공:
     - `POST /myoing_data/crawler/all` : 전체 크롤링 파이프라인 실행
     - `POST /myoing_data/crawler/basic` : 기본 크롤러만 실행
     - `POST /myoing_data/crawler/filter` : 필터링 작업만 실행
     - `POST /myoing_data/crawler/reviews` : 리뷰 크롤링만 실행
     - `POST /myoing_data/crawler/migrate` : DB 마이그레이션만 실행
     - `GET /myoing_data/crawler/status/{task_id}` : 작업 상태 확인
     - `GET /data/{data_type}` : 데이터 파일 목록 조회
     - `GET /data/{data_type}/{filename}` : 데이터 파일 내용 조회

3. Docker 방식: Docker Compose 실행
   - `docker-compose up` 명령어로 서비스 실행
   - 포트 7070으로 API 서버 접근 가능
   - MySQL 컨테이너 자동 실행 및 초기화

## CI/CD 파이프라인
GitHub Actions를 통한 자동화된 배포 프로세스:
1. 코드 품질 검사 (flake8)
2. Docker 이미지 빌드 및 푸시
3. 서버 배포 자동화

## 모듈화 진행 후 다시 되돌린 이유 
모듈화를 통해 공통 로직을 분리하고 재사용성을 높이고자 했으나, 실행 흐름상 모듈 간 의존성이 높고, 특히 driver_pool, initialize_driver_pool 등 공유 자원 제어 및 초기화에 있어 동기화 문제가 발생. 
각 모듈 내부에서 독립적으로 처리하기보다는 main.py에서 전반적인 흐름을 직접 통제하는 방식이 예외 처리와 상태 관리 측면에서 더 안정적이고 명확했음. 
결과적으로, 모듈화를 통한 분리보다 가독성과 유지보수를 고려한 단일 실행 컨트롤러 구조가 더 적절하다고 판단하여 기존 구조로 되돌림.